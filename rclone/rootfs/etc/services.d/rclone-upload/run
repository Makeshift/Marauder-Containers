#!/usr/bin/with-contenv sh
PREFIX=Media/
SOURCE=/shared/separate/upload_mount/upload/$PREFIX
MOUNT_LOCATION=/shared/merged/
DEST=uploadgdrive:/$PREFIX
CONF_FILE=/config/.rclone.conf
RCLONE_HOST="127.0.0.1"
RCLONE_PORT="5572"
RCLONE_ARGS="--fast-list -v --checkers 12 --buffer-size 16M --transfers 2 --use-mmap --low-level-retries 3 --multi-thread-cutoff 25M --multi-thread-streams 8 --no-update-modtime"
TMP_FILE=/tmp/rclone.uploads

LOG="[services.d] [rclone-upload]-$(s6-basename ${0}):"
DEBUG="${LOG} [DEBUG]:"

echo "$LOG Starting upload service"
while true; do 
    echo "$LOG Checking for uploads..."
    if [ -d "$SOURCE" ]; then
        # Get a static list of uploads
        UPLOAD_LIST=$(rclone --config "$CONF_FILE" lsf -R --files-only "$SOURCE" --min-age 1h)
        # Get dirs for the VFS refresh later
        DIR_LIST=$(rclone --config "$CONF_FILE" lsf -R --dirs-only "$SOURCE" --min-age 1h)
        # Count the bytes to check if we actually do have anything to upload
        COUNT=$(printf "%s" "$UPLOAD_LIST" | wc -c)
        if [ "$COUNT" -gt 0 ]; then
            # Print the static list to file
            printf "%s" "$UPLOAD_LIST" > "$TMP_FILE"
            # Copy everything to the mount first to ensure that it's all still available while we're uploading
            rclone --config "$CONF_FILE" copy --files-from "$TMP_FILE" "$SOURCE" "$DEST" --no-traverse $RCLONE_ARGS
            # Chill for 10s to make sure Gdrive is consistent
            sleep 10s
            # Do the 'move', which in theory will just delete the files on the local filesystem
            rclone --config "$CONF_FILE" move --files-from "$TMP_FILE" "$SOURCE" "$DEST" --delete-empty-src-dirs --no-traverse $RCLONE_ARGS
            echo "$LOG Upload complete"
            # Clean up after ourselves
            rm $TMP_FILE
            # Mass refresh everything we uploaded so nothing has a panic attack
            IFS='
            '
            for p in $DIR_LIST; do
                # Remove quotes
                p="${p%\"}"
                p="${p#\"}"
                # Add media folder prefix
                p=${PREFIX}${p}
                #echo "$DEBUG Telling Rclone @ ${RCLONE_HOST}:${RCLONE_PORT} to vfs/refresh ${p}"
                if curl -fs -X POST "${RCLONE_HOST}:${RCLONE_PORT}/vfs/refresh?dir=${p}"; then
                    #echo "$DEBUG Rclone VFS refreshed successfully: ${p}"
                    true
                else 
                    echo "$LOG Failed to VFS refresh, may not be visible to client: ${p}"
                fi

                #echo "$DEBUG Telling Rclone @ ${HOST}:${PORT} to cache/expire ${p}"
                if curl -fs -X POST "${HOST}:${PORT}/cache/expire?remote=${p}"; then
                    #echo "$DEBUG Rclone cache refreshed successfully: ${p}"
                    true
                else 
                    echo "$LOG Failed to cache refresh, may not be visible to client: ${p}"
                fi
            
            done
            # Now, force an ls in each dir to update the caches again
            #  but do it in the background so we don't slow down the script a bunch
            for p in $DIR_LIST; do
                echo "$DEBUG Force cache: $p"
                ls "${MOUNT_LOCATION}${PREFIX}${p}" >/dev/null &
            done
        else
            echo "$LOG Nothing to upload in $SOURCE"
        fi
    else
        echo "$LOG $SOURCE does not exist, skipping upload."
    fi
    # Clear vars for the next loop
    unset DIR_LIST UPLOAD_LIST COUNT
    # Sleep for a bit
    sleep 10m
done