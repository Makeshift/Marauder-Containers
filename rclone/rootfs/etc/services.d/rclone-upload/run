#!/usr/bin/with-contenv bash
set +x
set -o pipefail
PREFIX=Media/
SOURCE=/shared/separate/$PREFIX
MOUNT_LOCATION=/shared/merged/
MOUNT_NAME_PREFIX=gdrive_upload_crypt_
CONF_FILE=/config/.rclone.conf
RCLONE_HOST="127.0.0.1"
RCLONE_PORT="5572" # I should probably set this from an env var
SERVICE_ACCOUNT_PATH=/service_accounts
SERVICE_ACCOUNTS=$(find "${SERVICE_ACCOUNT_PATH}" -type f -name "*.json")
SERVICE_ACCOUNT_COUNT=$(printf "%s" "${SERVICE_ACCOUNTS}" | wc -l)
TMP_FILE=/tmp/rclone.uploads
TMP_LOG=/tmp/rclone.copylog
MIN_AGE=15m
SLEEP_SECONDS=600
IFS='
'
LOG="[services.d] [rclone-upload]-$(s6-basename "${0}"):"
DEBUG="${LOG} [DEBUG]:"
GLOBAL_ACCOUNT_NUM=0
TEAM_DRIVES_COUNT=$(printf "%s\n" ${rclone_team_drive_ids} | wc -l)
CURRENT_TEAM_DRIVE=0


rm $TMP_FILE > /dev/null 2>&1

echo "$LOG Starting upload service"
while true; do 
    START_TIME=$(date +%s)
    echo "$LOG Checking for uploads..."
    if [ -d "$SOURCE" ]; then
        # Get a static list of uploads
        UPLOAD_LIST=$(rclone --config "$CONF_FILE" lsf -R -L --files-only "$SOURCE" --min-age $MIN_AGE)
        # Get dirs for the VFS refresh later
        DIR_LIST=$(rclone --config "$CONF_FILE" lsf -R -L --dirs-only "$SOURCE" --min-age $MIN_AGE)
        # Count the bytes to check if we actually do have anything to upload
        COUNT_CHARS=$(printf "%s" "$UPLOAD_LIST" | wc -c)
        if [ "$COUNT_CHARS" -gt 0 ]; then
            # We got stuff to upload! Get a count and tell the user so they don't think we've hung
            #  also, this is sort of cheeky since it'll always return at least 1, but we already checked if we
            #  have stuff to upload so it's cool
            COUNT=$(printf "%s\n" "$UPLOAD_LIST" | wc -l)
            echo "$LOG Found ${COUNT} files to upload! Uploading..."
            # Print the static list to file
            printf "%s" "$UPLOAD_LIST" > "$TMP_FILE"
            
            # Start looping through service accounts
            CURRENT_ACCONT_NUM=0
            for ACCOUNT in $SERVICE_ACCOUNTS; do
                # Work out the current destination based on our team drive
                DEST=${MOUNT_NAME_PREFIX}${CURRENT_TEAM_DRIVE}:/$PREFIX
                # If we've burned through a couple of service accounts before, we can skip them
                CURRENT_ACCOUNT_NUM=$((CURRENT_ACCONT_NUM+1))
                if [ "$CURRENT_ACCOUNT_NUM" -lt "$GLOBAL_ACCOUNT_NUM" ]; then
                    # If we've burned through all our service accounts in this session, reset the global counter to 0
                    #  This means we'll exit the loop and not actually upload anything this time, but that just means we'll
                    #  tell rclone to clear caches unneccessarily and wait 10 minutes before uploading - oh well.
                    if [ "$SERVICE_ACCOUNT_COUNT" -eq "$GLOBAL_ACCOUNT_NUM" ]; then
                        GLOBAL_ACCOUNT_NUM=0
                    fi
                    continue
                fi
                # Copy everything to the mount first to ensure that it's all still available while we're uploading
                echo $LOG Trying service account $ACCOUNT
                echo $DEBUG rclone --config "$CONF_FILE" copy -L --files-from "$TMP_FILE" "$SOURCE" "$DEST" --drive-shared-with-me --no-traverse --drive-service-account-file "${ACCOUNT}" --drive-stop-on-upload-limit -v --buffer-size 16M --transfers 6 --use-mmap --low-level-retries 1 --multi-thread-cutoff 25M --multi-thread-streams 8 --no-update-modtime
                rclone --config "$CONF_FILE" copy -L --files-from "$TMP_FILE" "$SOURCE" "$DEST" --drive-shared-with-me --no-traverse --drive-service-account-file "${ACCOUNT}" --drive-stop-on-upload-limit -v --buffer-size 16M --transfers 6 --use-mmap --low-level-retries 1 --multi-thread-cutoff 25M --multi-thread-streams 8 --no-update-modtime 2>&1 | tee $TMP_LOG
                RCLONE_EXIT_CODE=$?
                if [ "${RCLONE_EXIT_CODE}" -ne "0" ] ; then
                    # Skip to the next service account. It might try to copy the same files twice, but that only costs us an API call.
                    echo $LOG Rclone exited ${RCLONE_EXIT_CODE} using account $ACCOUNT - Switching service account

                    # Rclone exited non-zero, check which error it is (We're going to switch service accounts anyway because it means less coding ;))
                    # TODO: Make it look out for the service account error properly
                    if grep "teamDriveFileLimitExceeded" $TMP_LOG > /dev/null; then
                        echo $LOG Rclone claims teamDriveFileLimitExceeded, switching team drive if possible
                        CURRENT_TEAM_DRIVE=$((CURRENT_TEAM_DRIVE+1))
                        if [ "$CURRENT_TEAM_DRIVE" -gt "$TEAM_DRIVES_COUNT" ]; then
                            CURRENT_TEAM_DRIVE=0
                        fi
                    fi
                    continue
                    
                fi
                # Chill for 10s to make sure Gdrive is consistent
                sleep 10s
                # Do the 'move', which in theory will just delete the files on the local filesystem. We can assume it'll never hit the transfer cap because
                #  it'll never actually do any moves.
                echo $DEBUG rclone --config "$CONF_FILE" move -L --files-from "$TMP_FILE" "$SOURCE" "$DEST" --drive-shared-with-me --fast-list --checkers 12 --delete-empty-src-dirs --drive-service-account-file "${ACCOUNT}" --no-traverse
                rclone --config "$CONF_FILE" move -L --files-from "$TMP_FILE" "$SOURCE" "$DEST" --drive-shared-with-me --fast-list --checkers 12 --delete-empty-src-dirs --drive-service-account-file "${ACCOUNT}" --no-traverse
                # If we get here, we can assume we've successfully copied and can kill the loop
                GLOBAL_ACCOUNT_NUM=$CURRENT_ACCOUNT_NUM
                break
            done
            echo "$LOG Upload complete"
            # Clean up after ourselves
            rm $TMP_FILE
            # Mass refresh everything we uploaded so nothing has a panic attack
            for p in $DIR_LIST; do
                # Add media folder prefix
                p=${PREFIX}${p}
                curl -fvsS  --output /dev/null --data-urlencode "dir=${p}" -X POST "${RCLONE_HOST}:${RCLONE_PORT}/vfs/refresh" 2>&1
                curl -fvsS --output /dev/null --data-urlencode "remote=${p}" -X POST "${RCLONE_HOST}:${RCLONE_PORT}/cache/expire" 2>&1
                ls "${MOUNT_LOCATION}${p}" >/dev/null &
            done
        else
            echo "$LOG Nothing to upload in $SOURCE"
        fi
    else
        echo "$LOG $SOURCE does not exist, skipping upload."
    fi
    # Clear vars for the next loop
    unset DIR_LIST UPLOAD_LIST COUNT
    # Take away the time we spent processing from the repeat time. This ensures we're only doing one upload at once, but if there's
    #  lots of files coming in, we'll continuously upload anything that matches the filter criteria
    END_TIME=$(date +%s)
    WORK_TIME=$(expr ${END_TIME} - ${START_TIME})
    SLEEP_TIME=$(expr ${SLEEP_SECONDS} - ${WORK_TIME})
    # Sleep for a bit if we have any sleeping to do
    if [ "$SLEEP_TIME" -gt 0 ]; then
        sleep ${SLEEP_TIME}s
    fi
done